{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us6uchlJsmhx"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets torch scikit-learn pandas\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the datasets\n",
        "hackaprompt_dataset = load_dataset('hackaprompt/hackaprompt-dataset')\n",
        "squad_dataset = load_dataset('rajpurkar/squad')\n",
        "deepset_dataset = load_dataset('deepset/prompt-injections')\n",
        "spml_dataset = load_dataset('reshabhs/SPML_Chatbot_Prompt_Injection')\n",
        "\n",
        "# Prepare prompts from each dataset\n",
        "injection_prompts = {\n",
        "    'SPML': [{'text': p, 'label': 1, 'source': 'SPML'} for p in spml_dataset['train']['User Prompt']],\n",
        "    'Hackaprompt': [{'text': p, 'label': 1, 'source': 'Hackaprompt'} for p in hackaprompt_dataset['train']['prompt']],\n",
        "    'Deepset': [{'text': p['text'], 'label': 1, 'source': 'Deepset'} for p in deepset_dataset['train'] if p['label'] == 1]\n",
        "}\n",
        "\n",
        "benign_prompts = {\n",
        "    'SPML': [{'text': p, 'label': 0, 'source': 'SPML'} for p in spml_dataset['train']['System Prompt']],\n",
        "    'SQuAD': [{'text': p, 'label': 0, 'source': 'SQuAD'} for p in squad_dataset['train']['question']],\n",
        "    'Deepset': [{'text': p['text'], 'label': 0, 'source': 'Deepset'} for p in deepset_dataset['train'] if p['label'] == 0]\n",
        "}\n",
        "\n",
        "# Function to sample prompts\n",
        "def sample_prompts(prompt_dict, n):\n",
        "    sampled = []\n",
        "    sources = list(prompt_dict.keys())\n",
        "    while len(sampled) < n:\n",
        "        source = random.choice(sources)\n",
        "        if prompt_dict[source]:\n",
        "            prompt = random.choice(prompt_dict[source])\n",
        "            sampled.append(prompt)\n",
        "            prompt_dict[source].remove(prompt)\n",
        "        else:\n",
        "            sources.remove(source)\n",
        "        if not sources:\n",
        "            break\n",
        "    return sampled\n",
        "\n",
        "# Determine number of prompts to sample\n",
        "total_prompts = 5000\n",
        "injection_prompts_count = int(total_prompts * 0.5)\n",
        "benign_prompts_count = total_prompts - injection_prompts_count\n",
        "\n",
        "# Sample prompts\n",
        "sampled_injection_prompts = sample_prompts(injection_prompts, injection_prompts_count)\n",
        "sampled_benign_prompts = sample_prompts(benign_prompts, benign_prompts_count)\n",
        "\n",
        "# If we don't have enough prompts, sample more from any source\n",
        "while len(sampled_injection_prompts) < injection_prompts_count:\n",
        "    source = random.choice(list(injection_prompts.keys()))\n",
        "    if injection_prompts[source]:\n",
        "        sampled_injection_prompts.append(injection_prompts[source].pop())\n",
        "\n",
        "while len(sampled_benign_prompts) < benign_prompts_count:\n",
        "    source = random.choice(list(benign_prompts.keys()))\n",
        "    if benign_prompts[source]:\n",
        "        sampled_benign_prompts.append(benign_prompts[source].pop())\n",
        "\n",
        "# Combine and shuffle the final dataset\n",
        "sampled_prompts = sampled_injection_prompts + sampled_benign_prompts\n",
        "random.shuffle(sampled_prompts)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(sampled_prompts)\n",
        "\n",
        "# Verify the counts and ratio\n",
        "injection_count = sum(1 for item in sampled_prompts if item['label'] == 1)\n",
        "benign_count = sum(1 for item in sampled_prompts if item['label'] == 0)\n",
        "\n",
        "print(f\"Number of injection prompts: {injection_count}\")\n",
        "print(f\"Number of benign prompts: {benign_count}\")\n",
        "print(f\"Percentage of injection prompts: {injection_count / total_prompts * 100:.2f}%\")\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df[['text', 'label']].to_csv('updated_prompt_injections.csv', index=False)\n",
        "\n",
        "print(\"Updated dataset saved as 'updated_prompt_injections.csv'\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Count prompts from each source\n",
        "for source in set(item['source'] for item in sampled_prompts):\n",
        "    count = sum(1 for item in sampled_prompts if item['source'] == source)\n",
        "    print(f\"Prompts from {source}: {count}\")\n",
        "\n",
        "# Print label distribution for each source\n",
        "print(\"\\nLabel distribution for each source:\")\n",
        "for source in set(item['source'] for item in sampled_prompts):\n",
        "    injection_count = sum(1 for item in sampled_prompts if item['source'] == source and item['label'] == 1)\n",
        "    benign_count = sum(1 for item in sampled_prompts if item['source'] == source and item['label'] == 0)\n",
        "    print(f\"{source}: Injection = {injection_count}, Benign = {benign_count}\")"
      ],
      "metadata": {
        "id": "ox7gR_rBspSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load custom dataset from CSV\n",
        "custom_dataset = pd.read_csv('updated_prompt_injections.csv')\n",
        "\n",
        "# Ensure the 'text' column contains strings\n",
        "custom_dataset['text'] = custom_dataset['text'].astype(str)\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(custom_dataset)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "# Using a 50-50 split as per your code\n",
        "train_dataset, test_dataset = train_test_split(dataset, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert split datasets back to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame(train_dataset))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame(test_dataset))\n",
        "\n",
        "# Load tokenizer\n",
        "model_name = 'albert-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
        "\n",
        "# Apply tokenize function\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove unneeded columns and set format for PyTorch\n",
        "columns_to_return = ['input_ids', 'attention_mask', 'label']\n",
        "train_dataset.set_format(type='torch', columns=columns_to_return)\n",
        "test_dataset.set_format(type='torch', columns=columns_to_return)\n",
        "\n",
        "# Print some information about the datasets\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(f\"Sample from train dataset: {train_dataset[0]}\")\n",
        "print(f\"Sample from test dataset: {test_dataset[0]}\")"
      ],
      "metadata": {
        "id": "yU5r1zvwsslo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer, AlbertConfig\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "\n",
        "def measure_time(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"{func.__name__} took {end_time - start_time:.2f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@measure_time\n",
        "def train_and_evaluate(params, train_dataset, test_dataset):\n",
        "    model_name = \"albert-base-v2\"\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "    config = AlbertConfig.from_pretrained(model_name)\n",
        "    config.num_labels = 2  # Binary classification\n",
        "    model = AlbertForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    eval_batch_size = 32\n",
        "    seed = 42\n",
        "    betas = (0.9, 0.999)\n",
        "    epsilon = 1e-08\n",
        "    lr_scheduler_warmup_steps = 500\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=params['train_batch_size'])\n",
        "    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=eval_batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=params['learning_rate'], betas=betas, eps=epsilon)\n",
        "\n",
        "    num_epochs = params['num_epochs']\n",
        "    total_steps = len(train_dataloader) * num_epochs // params['accumulation_steps']\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=lr_scheduler_warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(enabled=torch.cuda.is_available()):\n",
        "                outputs = model(**inputs, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss = loss / params['accumulation_steps']\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (step + 1) % params['accumulation_steps'] == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * params['accumulation_steps']  # Undo the division\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {avg_train_loss}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast(enabled=torch.cuda.is_available()):\n",
        "                outputs = model(**inputs, labels=labels)\n",
        "                logits = outputs.logits\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-6, 1e-3),\n",
        "        'train_batch_size': trial.suggest_categorical('train_batch_size', [4, 8, 16, 32]),\n",
        "        'accumulation_steps': trial.suggest_int('accumulation_steps', 1, 8),\n",
        "    }\n",
        "\n",
        "    max_epochs = 3\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        params['num_epochs'] = epoch + 1\n",
        "        accuracy, _ = train_and_evaluate(params, train_dataset, test_dataset)\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "\n",
        "    return best_accuracy\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Starting the process...\")\n",
        "\n",
        "baseline_params = {\n",
        "    'learning_rate': 1e-3,\n",
        "    'train_batch_size': 2,\n",
        "    'num_epochs': 3,\n",
        "    'accumulation_steps': 1\n",
        "}\n",
        "\n",
        "print(\"Evaluating baseline model...\")\n",
        "baseline_accuracy, baseline_report = train_and_evaluate(baseline_params, train_dataset, test_dataset)\n",
        "\n",
        "print(f\"\\nBaseline model accuracy: {baseline_accuracy:.4f}\")\n",
        "print(\"\\nBaseline Classification Report:\")\n",
        "print(baseline_report)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    study_name=\"Hyperband\",\n",
        "    direction=\"maximize\",\n",
        "    sampler=optuna.samplers.TPESampler(),\n",
        "    pruner=optuna.pruners.HyperbandPruner(\n",
        "        min_resource=1,\n",
        "        max_resource=5,\n",
        "        reduction_factor=3\n",
        "    )\n",
        ")\n",
        "\n",
        "n_trials = 5\n",
        "\n",
        "print(\"\\nRunning Hyperband optimization...\")\n",
        "optimization_start_time = time.time()\n",
        "study.optimize(objective, n_trials=n_trials)\n",
        "optimization_end_time = time.time()\n",
        "print(f\"Optimization took {optimization_end_time - optimization_start_time:.2f} seconds\")\n",
        "\n",
        "print(\"\\nHyperband optimization results:\")\n",
        "print(f\"  Best value: {study.best_value:.4f}\")\n",
        "print(\"  Best params:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "values = [t.value for t in study.trials if t.value is not None]\n",
        "print(f\"  Mean: {np.mean(values):.4f}\")\n",
        "print(f\"  Std: {np.std(values):.4f}\")\n",
        "\n",
        "best_params = study.best_params.copy()\n",
        "best_params['num_epochs'] = 3\n",
        "print(\"\\nEvaluating best model...\")\n",
        "best_accuracy, best_report = train_and_evaluate(best_params, train_dataset, test_dataset)\n",
        "\n",
        "print('\\nBest Model Classification Report:')\n",
        "print(best_report)\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Baseline accuracy: {baseline_accuracy:.4f}\")\n",
        "print(f\"Best model accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"Improvement: {best_accuracy - baseline_accuracy:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "18Ey09qIvnnp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}